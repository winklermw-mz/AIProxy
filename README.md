# Hybrid OpenAI-Compatible Inference Proxy

## Overview

This project is a **Flask-based API proxy** that provides an OpenAI-compatible interface for:

- Chat completions with automatic fallback, endpoint `/v1/chat/completions`
- Embeddings (local-only), endpoint `/v1/embeddings`
- Health checks, endpoint `/v1/heartbeat`

It attempts to use GitHub Models for inference first and falls back to a local LM Studioâ€“compatible server if GitHub inference fails due to rate limits for example.

## Availabe Model Providers

### GitHub
- Model: `openai/gpt-4.1-nano` (fixed intentionally, i.e. cannot be selected using the `model` parameter)
- Endpoint: `https://models.github.ai/inference`
- Auth: `GITHUB_TOKEN`

Please note, that embeddings are not available for GitHub models. Thus, this endpoint always falls back to the local model.

### Local Provider
- Chat Completion: `qwen/qwen3-vl-4b` by default, other models can be selected using the `model` parameter
- Embeddings: `text-embedding-jina-embeddings-v2-base-de` by default, other models can be selected using the `model` parameter
- Endpoint: `http://localhost:1234/v1`, which is the default endpoint for LM Studio
- Auth: no access token needed

## Endpoints

### Heartbeat

This endpoint does not take any parameters and accepts both `POST` and `GET` requests. The result is

```json
{
  "message": "running"
}
```

Please note, that this service only checks whether the proxy is available, but not whether the model providers are available.

### Chat Completions

This endpoint accepts `POST` requests of the following form:

```json
{
  "model": "qwen/qwen3-vl-4b",
  "messages": [
    { "role": "system", "content": "You are helpful." },
    { "role": "user", "content": "Hello!" }
  ]
}
```

The result looks like this:
```json
{
  "text": "Hello! How can I help you today?"
}
```

### Embeddings

This endpoint accepts `POST` requests of the following form:

```json
{
  "model": "text-embedding-jina-embeddings-v2-base-de",
  "input": "Some text"
}
```

The result just contains the embedding vector as list of floats.
```json
[0.0123, -0.9821, 0.4412, ...]
```

## Requirements

- Python 3.9+
- GitHub Models access
- Local inference server (optional but recommended)

Python Dependencies:

```bash
pip install flask openai python-dotenv
```

The GitHub access token must be set by

```bash
export GITHUB_TOKEN=your_github_models_api_key
```

## Running the Service

The service is started with the following command:

```bash
python app.py
```

The service will start on `http://localhost:8001`.